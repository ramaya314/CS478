{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbpLewOQ-k2w"
      },
      "source": [
        "# Assignment 4: Build a BERT-based Sentiment Classification (100 Points)\n",
        "\n",
        "Instructor: Ziyu Yao; Class: CS478 Fall 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fDUz9E-k2x"
      },
      "source": [
        "This assignment will extend from the last one. The goal is to build a binary sentiment classifier based on BERT. We will use Hugging Face transformers library (https://huggingface.co/docs/transformers/index). The assignment has two parts:\n",
        "- **Part 1 (70 points):** Evaluate a public, BERT-based sentiment classifier on our movie review sentiment dataset. In this part, no fine-tuning is needed.\n",
        "- **Part 2 (30 points):** Further fine-tune the pre-trained classifier on our sentiment classification dataset, hoping that it will give us a better accuracy on our movie review dataset.\n",
        "\n",
        "Since we will be fine-tuning a BERT model, you are suggested to use Google Colab and set up a GPU card (free resource). However, the assignment can be completed locally as well, although the training will take more time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTMzeL5A-k2y"
      },
      "source": [
        "To get started, please make sure to install the transformers library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wUZOfuDR-k2y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting notebook (from jupyter)\n",
            "  Downloading notebook-6.5.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting jupyter-console (from jupyter)\n",
            "  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nbconvert (from jupyter)\n",
            "  Downloading nbconvert-7.6.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: ipykernel in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyter) (6.16.2)\n",
            "Collecting jupyterlab (from jupyter)\n",
            "  Downloading jupyterlab-3.6.8-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting comm>=0.1.3 (from ipywidgets)\n",
            "  Downloading comm-0.1.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipywidgets) (5.9.0)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in c:\\program files\\python37\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (40.8.0)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: decorator in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
            "Requirement already satisfied: pygments in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: backcall in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: colorama in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: debugpy>=1.0 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (1.7.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (7.4.9)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (24.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (6.0.0)\n",
            "Requirement already satisfied: pyzmq>=17 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (26.2.0)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from ipykernel->jupyter) (6.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-console->jupyter) (4.12.0)\n",
            "Collecting jupyterlab-server~=2.19 (from jupyterlab->jupyter)\n",
            "  Downloading jupyterlab_server-2.24.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server<3,>=1.16.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_server-1.24.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting jupyter-ydoc~=0.2.4 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_ydoc-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jupyter-server-ydoc~=0.8.0 (from jupyterlab->jupyter)\n",
            "  Downloading jupyter_server_ydoc-0.8.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting nbclassic (from jupyterlab->jupyter)\n",
            "  Downloading nbclassic-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jinja2>=2.1 (from jupyterlab->jupyter)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting tomli (from jupyterlab->jupyter)\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting argon2-cffi (from notebook->jupyter)\n",
            "  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting ipython-genutils (from notebook->jupyter)\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n",
            "Collecting nbformat (from notebook->jupyter)\n",
            "  Downloading nbformat-5.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting Send2Trash>=1.8.0 (from notebook->jupyter)\n",
            "  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting terminado>=0.8.3 (from notebook->jupyter)\n",
            "  Downloading terminado-0.17.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting prometheus-client (from notebook->jupyter)\n",
            "  Downloading prometheus_client-0.17.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting beautifulsoup4 (from nbconvert->jupyter)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting bleach!=5.0.0 (from nbconvert->jupyter)\n",
            "  Downloading bleach-6.0.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting defusedxml (from nbconvert->jupyter)\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting importlib-metadata>=3.6 (from nbconvert->jupyter)\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting jupyterlab-pygments (from nbconvert->jupyter)\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting markupsafe>=2.0 (from nbconvert->jupyter)\n",
            "  Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl.metadata (3.1 kB)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting nbclient>=0.5.0 (from nbconvert->jupyter)\n",
            "  Downloading nbclient-0.7.4-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)\n",
            "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting tinycss2 (from nbconvert->jupyter)\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\program files\\python37\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.15.0)\n",
            "Collecting webencodings (from bleach!=5.0.0->nbconvert->jupyter)\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\program files\\python37\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\program files\\python37\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.7.4.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: entrypoints in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
            "Requirement already satisfied: pywin32>=1.0 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter) (306)\n",
            "Collecting anyio<4,>=3.1.0 (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter)\n",
            "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting websocket-client (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter)\n",
            "  Downloading websocket_client-1.6.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pywinpty (from jupyter-server<3,>=1.16.0->jupyterlab->jupyter)\n",
            "  Downloading pywinpty-2.0.10-cp37-none-win_amd64.whl.metadata (5.2 kB)\n",
            "Collecting jupyter-server-fileid<1,>=0.6.0 (from jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading jupyter_server_fileid-0.9.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting ypy-websocket<0.9.0,>=0.8.2 (from jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading ypy_websocket-0.8.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting y-py<0.7.0,>=0.6.0 (from jupyter-ydoc~=0.2.4->jupyterlab->jupyter)\n",
            "  Downloading y_py-0.6.2-cp37-none-win_amd64.whl.metadata (5.7 kB)\n",
            "Collecting babel>=2.10 (from jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading Babel-2.14.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading json5-0.9.16-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jsonschema>=4.17.3 (from jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: requests>=2.28 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyterlab-server~=2.19->jupyterlab->jupyter) (2.31.0)\n",
            "Collecting notebook-shim>=0.2.3 (from nbclassic->jupyterlab->jupyter)\n",
            "  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting fastjsonschema (from nbformat->notebook->jupyter)\n",
            "  Downloading fastjsonschema-2.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Collecting argon2-cffi-bindings (from argon2-cffi->notebook->jupyter)\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)\n",
            "  Downloading soupsieve-2.4.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter) (3.10)\n",
            "Collecting sniffio>=1.1 (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting exceptiongroup (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab->jupyter)\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pytz>=2015.7 (from babel>=2.10->jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting attrs>=17.4.0 (from jsonschema>=4.17.3->jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting importlib-resources>=1.4.0 (from jsonschema>=4.17.3->jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema>=4.17.3->jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl.metadata (624 bytes)\n",
            "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema>=4.17.3->jupyterlab-server~=2.19->jupyterlab->jupyter)\n",
            "  Downloading pyrsistent-0.19.3-cp37-cp37m-win_amd64.whl.metadata (975 bytes)\n",
            "Collecting jupyter-events>=0.5.0 (from jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading jupyter_events-0.6.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.28->jupyterlab-server~=2.19->jupyterlab->jupyter) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.28->jupyterlab-server~=2.19->jupyterlab->jupyter) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from requests>=2.28->jupyterlab-server~=2.19->jupyterlab->jupyter) (2024.8.30)\n",
            "Collecting aiofiles<23,>=22.1.0 (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading aiofiles-22.1.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting aiosqlite<1,>=0.17.0 (from ypy-websocket<0.9.0,>=0.8.2->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading aiosqlite-0.19.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter)\n",
            "  Downloading cffi-1.15.1-cp37-cp37m-win_amd64.whl.metadata (1.1 kB)\n",
            "Collecting typing-extensions>=3.6.4 (from importlib-metadata>=3.6->nbconvert->jupyter)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter)\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\malma\\appdata\\roaming\\python\\python37\\site-packages (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter) (6.0.1)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading webcolors-1.13-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting cached-property>=1.3.0 (from fqdn->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.5.0->jupyter-server-fileid<1,>=0.6.0->jupyter-server-ydoc~=0.8.0->jupyterlab->jupyter)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "   ---------------------------------------- 139.8/139.8 kB 4.2 MB/s eta 0:00:00\n",
            "Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
            "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
            "   ---------------------------------------- 214.4/214.4 kB ? eta 0:00:00\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "   ---------------------------------------- 2.3/2.3 MB 49.5 MB/s eta 0:00:00\n",
            "Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading jupyterlab-3.6.8-py3-none-any.whl (8.9 MB)\n",
            "   ---------------------------------------- 8.9/8.9 MB 51.8 MB/s eta 0:00:00\n",
            "Downloading notebook-6.5.7-py3-none-any.whl (529 kB)\n",
            "   --------------------------------------- 529.8/529.8 kB 32.5 MB/s eta 0:00:00\n",
            "Downloading nbconvert-7.6.0-py3-none-any.whl (290 kB)\n",
            "   --------------------------------------- 290.4/290.4 kB 17.5 MB/s eta 0:00:00\n",
            "Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
            "   --------------------------------------- 162.5/162.5 kB 10.2 MB/s eta 0:00:00\n",
            "Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "   ---------------------------------------- 133.3/133.3 kB ? eta 0:00:00\n",
            "Downloading jupyter_server-1.24.0-py3-none-any.whl (347 kB)\n",
            "   --------------------------------------- 347.5/347.5 kB 22.5 MB/s eta 0:00:00\n",
            "Downloading jupyter_server_ydoc-0.8.0-py3-none-any.whl (11 kB)\n",
            "Downloading jupyter_ydoc-0.2.5-py3-none-any.whl (6.2 kB)\n",
            "Downloading jupyterlab_server-2.24.0-py3-none-any.whl (57 kB)\n",
            "   ---------------------------------------- 57.3/57.3 kB 2.9 MB/s eta 0:00:00\n",
            "Downloading MarkupSafe-2.1.5-cp37-cp37m-win_amd64.whl (17 kB)\n",
            "Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "   ---------------------------------------- 48.0/48.0 kB ? eta 0:00:00\n",
            "Downloading nbclassic-1.1.0-py3-none-any.whl (10.0 MB)\n",
            "   ---------------------------------------- 10.0/10.0 MB 53.4 MB/s eta 0:00:00\n",
            "Downloading nbclient-0.7.4-py3-none-any.whl (73 kB)\n",
            "   ---------------------------------------- 73.1/73.1 kB 3.9 MB/s eta 0:00:00\n",
            "Downloading nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
            "   ---------------------------------------- 77.4/77.4 kB ? eta 0:00:00\n",
            "Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
            "Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
            "Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "   ---------------------------------------- 147.9/147.9 kB 9.2 MB/s eta 0:00:00\n",
            "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\n",
            "   ---------------------------------------- 60.6/60.6 kB ? eta 0:00:00\n",
            "Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "   ---------------------------------------- 80.9/80.9 kB ? eta 0:00:00\n",
            "Downloading Babel-2.14.0-py3-none-any.whl (11.0 MB)\n",
            "   ---------------------------------------- 11.0/11.0 MB 54.7 MB/s eta 0:00:00\n",
            "Downloading json5-0.9.16-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "   ---------------------------------------- 90.4/90.4 kB 5.0 MB/s eta 0:00:00\n",
            "Downloading jupyter_server_fileid-0.9.3-py3-none-any.whl (16 kB)\n",
            "Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
            "Downloading pywinpty-2.0.10-cp37-none-win_amd64.whl (1.4 MB)\n",
            "   ---------------------------------------- 1.4/1.4 MB 45.2 MB/s eta 0:00:00\n",
            "Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
            "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading y_py-0.6.2-cp37-none-win_amd64.whl (549 kB)\n",
            "   --------------------------------------- 549.4/549.4 kB 33.7 MB/s eta 0:00:00\n",
            "Downloading ypy_websocket-0.8.4-py3-none-any.whl (10 kB)\n",
            "Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
            "Downloading fastjsonschema-2.20.0-py3-none-any.whl (23 kB)\n",
            "Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
            "   ---------------------------------------- 56.9/56.9 kB ? eta 0:00:00\n",
            "Downloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
            "Downloading aiosqlite-0.19.0-py3-none-any.whl (15 kB)\n",
            "Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "   ---------------------------------------- 63.0/63.0 kB ? eta 0:00:00\n",
            "Downloading cffi-1.15.1-cp37-cp37m-win_amd64.whl (179 kB)\n",
            "   --------------------------------------- 179.3/179.3 kB 10.6 MB/s eta 0:00:00\n",
            "Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
            "Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Downloading pyrsistent-0.19.3-cp37-cp37m-win_amd64.whl (62 kB)\n",
            "   ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "   --------------------------------------- 508.0/508.0 kB 33.2 MB/s eta 0:00:00\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "   ---------------------------------------- 118.7/118.7 kB 6.8 MB/s eta 0:00:00\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "   ---------------------------------------- 66.4/66.4 kB 3.5 MB/s eta 0:00:00\n",
            "Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: y-py, webencodings, pytz, json5, ipython-genutils, fastjsonschema, cached-property, widgetsnbextension, websocket-client, webcolors, uri-template, typing-extensions, tomli, tinycss2, soupsieve, sniffio, Send2Trash, rfc3986-validator, rfc3339-validator, pywinpty, python-json-logger, pyrsistent, pycparser, prometheus-client, pkgutil-resolve-name, pandocfilters, mistune, markupsafe, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, importlib-resources, fqdn, exceptiongroup, defusedxml, comm, bleach, babel, aiofiles, terminado, jinja2, importlib-metadata, cffi, beautifulsoup4, arrow, anyio, aiosqlite, ypy-websocket, jupyter-ydoc, isoduration, ipywidgets, attrs, argon2-cffi-bindings, jupyter-console, jsonschema, argon2-cffi, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-server-fileid, nbclassic, jupyter-server-ydoc, notebook, jupyterlab, jupyter\n",
            "Successfully installed Send2Trash-1.8.3 aiofiles-22.1.0 aiosqlite-0.19.0 anyio-3.7.1 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 attrs-24.2.0 babel-2.14.0 beautifulsoup4-4.12.3 bleach-6.0.0 cached-property-1.5.2 cffi-1.15.1 comm-0.1.4 defusedxml-0.7.1 exceptiongroup-1.2.2 fastjsonschema-2.20.0 fqdn-1.5.1 importlib-metadata-6.7.0 importlib-resources-5.12.0 ipython-genutils-0.2.0 ipywidgets-8.1.5 isoduration-20.11.0 jinja2-3.1.4 json5-0.9.16 jsonpointer-3.0.0 jsonschema-4.17.3 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.6.3 jupyter-server-1.24.0 jupyter-server-fileid-0.9.3 jupyter-server-ydoc-0.8.0 jupyter-ydoc-0.2.5 jupyterlab-3.6.8 jupyterlab-pygments-0.2.2 jupyterlab-server-2.24.0 jupyterlab-widgets-3.0.13 markupsafe-2.1.5 mistune-3.0.2 nbclassic-1.1.0 nbclient-0.7.4 nbconvert-7.6.0 nbformat-5.8.0 notebook-6.5.7 notebook-shim-0.2.4 pandocfilters-1.5.1 pkgutil-resolve-name-1.3.10 prometheus-client-0.17.1 pycparser-2.21 pyrsistent-0.19.3 python-json-logger-2.0.7 pytz-2024.2 pywinpty-2.0.10 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 sniffio-1.3.1 soupsieve-2.4.1 terminado-0.17.1 tinycss2-1.2.1 tomli-2.0.1 typing-extensions-4.7.1 uri-template-1.3.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.6.1 widgetsnbextension-4.0.13 y-py-0.6.2 ypy-websocket-0.8.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rfclQSOg-k2z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from typing import List, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# transformers\n",
        "import transformers\n",
        "\n",
        "# Set up overall seed\n",
        "seed = 12345\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKClm8sw-k2z"
      },
      "source": [
        "## Part 0: Data Structure and Loading\n",
        "\n",
        "This part is adapted from the previous assignment, including reusing the data structure and evaluation scripts that we have previously defined (but with modifications). _There's nothing to fill out in this part, but please study the code and particularly check the modifications that we made to accommodate the BERT classifier._\n",
        "\n",
        "First, we will define the data structure for storing our sentiment data. Compared with the previous version, we have removed the naive sentence tokenization based on whitespace split. Later on, we will use the tokenizer of the pre-trained model for sentence tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8oWPkapg-k2z"
      },
      "outputs": [],
      "source": [
        "class SentimentExample:\n",
        "    \"\"\"\n",
        "    Data wrapper for a single example for sentiment analysis.\n",
        "\n",
        "    Attributes:\n",
        "        sentence (string): a string-type sentence (untokenized)\n",
        "        label (int): 0 or 1 (0 = negative, 1 = positive)\n",
        "        word_indices (List[int]): list of word indices in the vocab, which will generated by the `indexing_sentiment_examples` method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentence, label):\n",
        "        self.sentence = sentence\n",
        "        self.label = label\n",
        "        self.words = None\n",
        "        self.word_indices = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.sentence + \"; label=\" + repr(self.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "def read_sentiment_examples(infile: str) -> List[SentimentExample]:\n",
        "    \"\"\"\n",
        "    Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences and forms\n",
        "    SentimentExamples. Note that all words have been lowercased.\n",
        "\n",
        "    :param infile: file to read from\n",
        "    :return: a list of SentimentExamples parsed from the file\n",
        "    \"\"\"\n",
        "    f = open(infile)\n",
        "    exs = []\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0:\n",
        "            line = line.strip()\n",
        "            fields = line.split(\"\\t\")\n",
        "            if len(fields) != 2:\n",
        "                fields = line.split()\n",
        "                label = 0 if \"0\" in fields[0] else 1\n",
        "                sent = \" \".join(fields[1:])\n",
        "            else:\n",
        "                # Slightly more robust to reading bad output than int(fields[0])\n",
        "                label = 0 if \"0\" in fields[0] else 1\n",
        "                sent = fields[1]\n",
        "            sent = sent.lower() # lowercasing\n",
        "            exs.append(SentimentExample(sent, label))\n",
        "    f.close()\n",
        "    return exs\n",
        "\n",
        "\n",
        "def read_blind_sst_examples(infile: str) -> List[SentimentExample]:\n",
        "    \"\"\"\n",
        "    Reads the blind SST test set, which just consists of unlabeled sentences. Note that all words have been lowercased.\n",
        "    :param infile: path to the file to read\n",
        "    :return: list of tokenized sentences (list of list of strings)\n",
        "    \"\"\"\n",
        "    f = open(infile, encoding='utf-8')\n",
        "    exs = []\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0:\n",
        "            line = line.strip()\n",
        "            sent = line.lower()\n",
        "            exs.append(SentimentExample(sent, label=-1)) # pseudo label -1\n",
        "    return exs\n",
        "\n",
        "\n",
        "def write_sentiment_examples(exs: List[SentimentExample], outfile: str):\n",
        "    \"\"\"\n",
        "    Writes sentiment examples to an output file with one example per line, the predicted label followed by the example.\n",
        "    Note that what gets written out is tokenized.\n",
        "    :param exs: the list of SentimentExamples to write\n",
        "    :param outfile: out path\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    o = open(outfile, 'w')\n",
        "    for ex in exs:\n",
        "        o.write(repr(ex.label) + \"\\t\" + ex.sentence + \"\\n\")\n",
        "    o.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEWmDN62-k20"
      },
      "source": [
        "Now, load the training, dev, and test sets (the same sets as in the prior assignment):\n",
        "\n",
        "(Note: The following commands is for Google Colab users only. If you run the notebook locally, change the path configuration accordingly.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaS6x68N_Lbi"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KA4Ts_2n-k20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6920 / 872 / 1821 train/dev/test examples\n"
          ]
        }
      ],
      "source": [
        "# Specify the data paths\n",
        "train_path = \"H:\\\\Projects\\\\CS478\\\\Assignment3\\\\data\\\\train.txt\"\n",
        "dev_path = \"H:\\\\Projects\\\\CS478\\\\Assignment3\\\\data\\\\dev.txt\"\n",
        "blind_test_path = \"H:\\\\Projects\\\\CS478\\\\Assignment3\\\\data\\\\test-blind.txt\" # blind test\n",
        "\n",
        "# Load train, dev, and test exs and index the words.\n",
        "train_exs = read_sentiment_examples(train_path)\n",
        "dev_exs = read_sentiment_examples(dev_path)\n",
        "test_exs_words_only = read_blind_sst_examples(blind_test_path)\n",
        "print(repr(len(train_exs)) + \" / \" + repr(len(dev_exs)) + \" / \" + repr(len(test_exs_words_only)) + \" train/dev/test examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wML3Dm5y-k20"
      },
      "source": [
        "Next, we set up the evaluation metrics for sentiment classification. The function is the same as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c1U7Y88l-k20"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(golds: List[int], predictions: List[int], print_only: bool=False):\n",
        "    \"\"\"\n",
        "    Calculate evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
        "    Returns accuracy, precision, recall, and F1.\n",
        "\n",
        "    :param golds: gold labels\n",
        "    :param predictions: pred labels\n",
        "    :param print_only: set to True if printing the stats without returns\n",
        "    :return: accuracy, precision, recall, and F1 (all floating numbers), or None (when print_only is True)\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    num_pos_correct = 0\n",
        "    num_pred = 0\n",
        "    num_gold = 0\n",
        "    num_total = 0\n",
        "    if len(golds) != len(predictions):\n",
        "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(golds), len(predictions)))\n",
        "    for idx in range(0, len(golds)):\n",
        "        gold = golds[idx]\n",
        "        prediction = predictions[idx]\n",
        "        if prediction == gold:\n",
        "            num_correct += 1\n",
        "        if prediction == 1:\n",
        "            num_pred += 1\n",
        "        if gold == 1:\n",
        "            num_gold += 1\n",
        "        if prediction == 1 and gold == 1:\n",
        "            num_pos_correct += 1\n",
        "        num_total += 1\n",
        "    acc = float(num_correct) / num_total\n",
        "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
        "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
        "    f1 = 2 * prec * rec / (prec + rec) if prec > 0 and rec > 0 else 0.0\n",
        "\n",
        "    print(\"Accuracy: %i / %i = %f\" % (num_correct, num_total, acc))\n",
        "    print(\"Precision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
        "          + \"; Recall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
        "          + \"; F1 (harmonic mean of precision and recall): %f\" % f1)\n",
        "\n",
        "    if not print_only:\n",
        "        return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVRJUGN-k21"
      },
      "source": [
        "## Part 1: Evaluate a Pre-trained Sentiment Classifier (70 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8KwNsMC-k21"
      },
      "source": [
        "In this part, we will evaluate a pre-trained sentiment classifier (\"textattack/bert-base-uncased-yelp-polarity\") on our sentiment classification dataset. This classifier, as shown by its name, was pre-trained on BERT (uncased base version) and then fine-tuned on a Yelp polarity dataset. It has two class labels, 0 representing negative and 1 representing positive. The model is open-sourced on the tranformers model hub (https://huggingface.co/textattack/bert-base-uncased-yelp-polarity). We will test how well the model performs on the dev set, without fine-tuning.\n",
        "\n",
        "To this end, we will do three things:\n",
        "- Create and load a tokenizer for the pre-trained model, using `BertTokenizer` (https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/bert#transformers.BertTokenizer);\n",
        "- Create and load the pre-trained classifier, using `BertForSequenceClassification` (https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/bert#transformers.BertForSequenceClassification);\n",
        "- Run this model on our dev set and report its accuracy.\n",
        "\n",
        "Please follow the instructions and fill out the missing code.\n",
        "\n",
        "**Recommended Material:** You may find the example code of `BertForSequenceClassification` helpful: https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/bert#transformers.BertForSequenceClassification.forward.example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ys161_z-k21"
      },
      "source": [
        "First, let's load the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-owuMYn8-k21"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5q93d8d-k22"
      },
      "source": [
        "Define the pre-trained checkpoint name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aXj5nb4N-k22"
      },
      "outputs": [],
      "source": [
        "pretrained_checkpoint = \"textattack/bert-base-uncased-yelp-polarity\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNHOkXty-k22"
      },
      "source": [
        "Follow the example here (https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/bert#transformers.BertForSequenceClassification.forward.example), create a tokenizer and load the pre-trained model of `textattack/bert-base-uncased-yelp-polarity`.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Complete the following code for creating the tokenizer and the pre-trained model.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "U1oAy7jq-k22",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 11.3MB/s]\n",
            "C:\\Users\\malma\\AppData\\Roaming\\Python\\Python37\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\malma\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 26.9kB/s]\n",
            "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 16.0kB/s]\n",
            "Downloading config.json: 100%|██████████| 520/520 [00:00<00:00, 144kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:03<00:00, 114MB/s]  \n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_checkpoint)\n",
        "model = BertForSequenceClassification.from_pretrained(pretrained_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdj3wePR-k22"
      },
      "source": [
        "As before, we can \"print\" the model and see its structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-265Xdav-k22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monYGJkW-k22"
      },
      "source": [
        "As before, we will project the model to the device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nvITzcun-k22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0VWAsU--k22"
      },
      "source": [
        "Now, let's see how to use this classifier. Consider the first sentence in our dev set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IZBdD6LQ-k22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "it 's a lovely film with lovely performances by buy and accorsi .; label=1"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_exs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKJQI5t8-k22"
      },
      "source": [
        "Tokenize the sentence using the created tokenizer:\n",
        "\n",
        "<font color='blue'>YOUR TASK: Complete the following code for tokenizing the first sentence in the dev set.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mn2neXIW-k23"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "inputs = tokenizer(dev_exs[0].sentence, return_tensors='pt', padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHkW2ZoG-k23"
      },
      "source": [
        "See what it contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "01JgaWUr-k23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2009,  1005,  1055,  1037,  8403,  2143,  2007,  8403,  4616,\n",
              "          2011,  4965,  1998, 16222,  5668,  2072,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBrr0m9Q-k23"
      },
      "source": [
        "The tokenization results contain three pieces:\n",
        "- `input_ids` of shape `(batch_size x batch_max_length)`, which is a tensor of the tokenized and indexed input sentences. Since there's only one sentence to the tokenizer, the first dimension (`batch_size`) is 1 and the second dimension is the length of the input sentence.\n",
        "- `token_type_ids` of shape `(batch_size x batch_max_length)`, which is an all-zero tensor because there's only one \"type\" of input (i.e., a single task input sentence).\n",
        "- `attention_mask` of shape `(batch_size x batch_max_length)`, with 1 indicating valid attention and 0 otherwise. Here the tensor is all ones because all words are valid for being attended.\n",
        "\n",
        "You should also see `device='cuda:0'` if you are using GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ieb3HvTj-k23"
      },
      "source": [
        "Now, run the model inference given this sentence. You should use the \"inputs\" defined above. The returned \"outputs\" should contain the model logits.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Complete the following code for classifying the first sentence in the dev set.</font>\n",
        "\n",
        "Hint: if you receive an error \"Expected all tensors to be on the same device\", please apply `.to(device)` to your `inputs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PMLG9QXq-k23"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYG4y_nH-k23"
      },
      "source": [
        "See what it contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "06Se_aVY-k23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-4.5182,  4.2525]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt__W8c--k23"
      },
      "source": [
        "The classifier output has multiple items. Among them:\n",
        "- `logits` is the logits (i.e., values before softmax) from the classifier.\n",
        "- `loss` is the model (cross entropy) loss on the given input, which is now `None` because there's no ground-truth labels provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-4uZ7Nr-k23"
      },
      "source": [
        "Calculate the prediction labels (1 for positive and 0 for negative) based on the model logits:\n",
        "\n",
        "<font color='blue'>YOUR TASK: Complete the following code for getting the prediction label.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WdOFPL-g-k23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: 1\n"
          ]
        }
      ],
      "source": [
        "# TODO:\n",
        "predicted_class_id = (torch.argmax(outputs.logits, dim=1)).item()\n",
        "print(\"Prediction:\", predicted_class_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RI0zpbt-k23"
      },
      "source": [
        "While the above code can process a single input sentence, to make the best use of the GPU, we want to batchify this process, including tokenizing a batch of input sentences, and running the loaded classifier to make predictions for the batch data.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Complete the `TODO` to implement `SentimentExampleBatchIterator`, which loads and tokenizes a batch of input data at a time.</font>\n",
        "\n",
        "**Hint:** You would need to specify the `padding` and `truncation` options for tokenizing a batch of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "NJPVHG27-k23"
      },
      "outputs": [],
      "source": [
        "class SentimentExampleBatchIterator:\n",
        "    \"\"\"\n",
        "    A batch iterator which will produce the next batch indexed data.\n",
        "\n",
        "    Attributes:\n",
        "        data: a list of SentimentExample objects, which is the source data input\n",
        "        batch_size: an integer number indicating the number of examples in each batch\n",
        "        PAD_idx: the index of PAD in the vocabulary\n",
        "        shuffle: whether to shuffle the data (should set to True only for training)\n",
        "    \"\"\"\n",
        "    def __init__(self, data: List[SentimentExample], batch_size: int, shuffle: bool=True):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self._indices = None\n",
        "        self._cur_idx = None\n",
        "\n",
        "    def refresh(self):\n",
        "        self._indices = list(range(len(self.data)))\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self._indices)\n",
        "        self._cur_idx = 0\n",
        "\n",
        "    def get_next_batch(self, tokenizer): # note that we now feed the tokenizer as an argument, which decides how a sentence needs to be tokenized\n",
        "        if self._cur_idx < len(self.data): # loop over the dataset\n",
        "            st_idx = self._cur_idx\n",
        "            if self._cur_idx + self.batch_size > len(self.data) - 1:\n",
        "                ed_idx = len(self.data)\n",
        "            else:\n",
        "                ed_idx = self._cur_idx + self.batch_size\n",
        "            self._cur_idx = ed_idx # update\n",
        "            # Retrieve a batch of SentimentExample data. Note that the data is NOT yet tokenized.\n",
        "            batch_exs = [self.data[self._indices[_idx]] for _idx in range(st_idx, ed_idx)]\n",
        "\n",
        "            # TODO: tokenize the `batch_exs` data. The resulting `batch_inputs` will have\n",
        "            # exactly the same fields as `inputs`, i.e., input_ids, token_type_ids, and attention_mask.\n",
        "            # However, you should see that now each field contains the result from a batch of data with paddings.\n",
        "            # Hint: https://huggingface.co/course/chapter2/2?fw=pt#preprocessing-with-a-tokenizer\n",
        "            batch_inputs = tokenizer([ex.sentence for ex in batch_exs], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "            # collect the human labels in this batch\n",
        "            batch_labels = torch.tensor(np.array([ex.label for ex in batch_exs], dtype=np.int64)).to(device)\n",
        "            return batch_inputs, batch_labels\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpd5HeY5-k28"
      },
      "source": [
        "<font color='blue'>YOUR TASK: Complete the `TODO` to make prediction on a batch of data each time, using the loaded pretrained classifier (the `classifier` argument).</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "T0e6Bl5j-k28"
      },
      "outputs": [],
      "source": [
        "def batch_predict(classifier, batch_inputs: torch.Tensor) -> List[int]:\n",
        "    # TODO: implement the batch prediction using the `classifier` (the loaded BERT-based model)\n",
        "    # The function should return a list of 1/0 predicted labels\n",
        "    with torch.no_grad():\n",
        "        outputs = classifier(**batch_inputs)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).tolist()\n",
        "    return preds\n",
        "\n",
        "def evaluate(classifier, exs: List[SentimentExample], return_metrics: bool=False):\n",
        "    \"\"\"\n",
        "    Evaluates a given classifier on the given examples\n",
        "    :param classifier: classifier to evaluate\n",
        "    :param exs: the list of SentimentExamples to evaluate on\n",
        "    :param return_metrics: set to True if returning the stats\n",
        "    :return: None (but prints output)\n",
        "    \"\"\"\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    eval_batch_iterator = SentimentExampleBatchIterator(exs, batch_size=32, shuffle=False) # hard-coded batch size\n",
        "    eval_batch_iterator.refresh()\n",
        "    batch_data = eval_batch_iterator.get_next_batch(tokenizer)\n",
        "    while batch_data is not None:\n",
        "        batch_inputs, batch_labels = batch_data\n",
        "\n",
        "        all_labels += list(batch_labels)\n",
        "\n",
        "        preds = batch_predict(classifier, batch_inputs)\n",
        "        all_preds += preds\n",
        "\n",
        "        batch_data = eval_batch_iterator.get_next_batch(tokenizer)\n",
        "\n",
        "    if return_metrics:\n",
        "        acc, prec, rec, f1 = calculate_metrics(all_labels, all_preds)\n",
        "        return acc, prec, rec, f1\n",
        "    else:\n",
        "        calculate_metrics(all_labels, all_preds, print_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NQopKtR-k29"
      },
      "source": [
        "Now, let's run the model and evaluate its performance on the dev set!\n",
        "\n",
        "(If your implementation is correct, you should see an accuracy of around 0.8.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "H4z56CBs-k29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 686 / 872 = 0.786697\n",
            "Precision (fraction of predicted positives that are correct): 357 / 456 = 0.782895; Recall (fraction of true positives predicted correctly): 357 / 444 = 0.804054; F1 (harmonic mean of precision and recall): 0.793333\n"
          ]
        }
      ],
      "source": [
        "evaluate(model, dev_exs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QysSDXXV-k29"
      },
      "source": [
        "## Part 2: Fine-tune a BERT-based Model for Sentiment Classification (30 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTGZ3oBO-k29"
      },
      "source": [
        "While in the last part we directly load in a sentiment classifier and use it in our movie review sentiment classification task, in this part, we will fine-tune this classifier on our dataset and see if it can achieve a better task performance in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqtdeU7--k29"
      },
      "source": [
        "Like before, we will create an optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "igdzSU7u-k29"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(),\n",
        "  lr = 5e-5, eps = 1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGmQJ2-c-k29"
      },
      "source": [
        "<font color='blue'>YOUR TASK: Complete the `TODO` for model training.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wCUS0KmG-k29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Avg loss: 0.37332\n",
            "Accuracy: 764 / 872 = 0.876147\n",
            "Precision (fraction of predicted positives that are correct): 376 / 416 = 0.903846; Recall (fraction of true positives predicted correctly): 376 / 444 = 0.846847; F1 (harmonic mean of precision and recall): 0.874419\n",
            "Secure a new best accuracy 0.876 in epoch 0!\n",
            "Save the best model checkpoint as `best_model.ckpt`!\n",
            "Time elapsed: 00h22m06s\n",
            "----------\n",
            "Epoch 1\n",
            "Avg loss: 0.13756\n",
            "Accuracy: 753 / 872 = 0.863532\n",
            "Precision (fraction of predicted positives that are correct): 364 / 403 = 0.903226; Recall (fraction of true positives predicted correctly): 364 / 444 = 0.819820; F1 (harmonic mean of precision and recall): 0.859504\n",
            "Time elapsed: 00h44m05s\n",
            "----------\n",
            "Epoch 2\n",
            "Avg loss: 0.04053\n",
            "Accuracy: 772 / 872 = 0.885321\n",
            "Precision (fraction of predicted positives that are correct): 386 / 428 = 0.901869; Recall (fraction of true positives predicted correctly): 386 / 444 = 0.869369; F1 (harmonic mean of precision and recall): 0.885321\n",
            "Secure a new best accuracy 0.885 in epoch 2!\n",
            "Save the best model checkpoint as `best_model.ckpt`!\n",
            "Time elapsed: 01h06m24s\n",
            "----------\n",
            "Epoch 3\n",
            "Avg loss: 0.03272\n",
            "Accuracy: 769 / 872 = 0.881881\n",
            "Precision (fraction of predicted positives that are correct): 404 / 467 = 0.865096; Recall (fraction of true positives predicted correctly): 404 / 444 = 0.909910; F1 (harmonic mean of precision and recall): 0.886937\n",
            "Time elapsed: 01h28m53s\n",
            "----------\n",
            "Epoch 4\n",
            "Avg loss: 0.01252\n",
            "Accuracy: 765 / 872 = 0.877294\n",
            "Precision (fraction of predicted positives that are correct): 404 / 471 = 0.857749; Recall (fraction of true positives predicted correctly): 404 / 444 = 0.909910; F1 (harmonic mean of precision and recall): 0.883060\n",
            "Time elapsed: 01h51m21s\n",
            "----------\n",
            "End of training! The best accuracy 0.885 was obtained in epoch 2.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 5\n",
        "\n",
        "# create a batch iterator for the training data\n",
        "batch_iterator = SentimentExampleBatchIterator(train_exs, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# training\n",
        "best_epoch = -1\n",
        "best_acc = -1\n",
        "start_time = time.time()\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(\"Epoch %i\" % epoch)\n",
        "\n",
        "    batch_iterator.refresh() # initiate a new iterator for this epoch\n",
        "\n",
        "    model.train() # turn on the \"training mode\"\n",
        "    batch_loss = 0.0\n",
        "    batch_example_count = 0\n",
        "    batch_data = batch_iterator.get_next_batch(tokenizer)\n",
        "    while batch_data is not None:\n",
        "        batch_inputs, batch_labels = batch_data\n",
        "        model.zero_grad()\n",
        "\n",
        "        # TODO: call the model and compute the loss\n",
        "        # Hint: you don't need to implement the loss on your own when using the transformers `BertForSequenceClassification`;\n",
        "        # see https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/bert#transformers.BertForSequenceClassification.forward.example for help\n",
        "        batch_outputs = model(**batch_inputs, labels=batch_labels)\n",
        "        loss = batch_outputs.loss\n",
        "\n",
        "        # record the loss and number of examples, so we could report some stats\n",
        "        batch_example_count += len(batch_labels)\n",
        "        batch_loss += loss.item() * len(batch_labels)\n",
        "\n",
        "        # backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # get another batch\n",
        "        batch_data = batch_iterator.get_next_batch(tokenizer)\n",
        "\n",
        "    print(\"Avg loss: %.5f\" % (batch_loss / batch_example_count))\n",
        "\n",
        "    # evaluate on dev set\n",
        "    model.eval() # turn on the \"evaluation mode\"\n",
        "    acc, _, _, _ = evaluate(model, dev_exs, return_metrics=True)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_epoch = epoch\n",
        "        print(\"Secure a new best accuracy %.3f in epoch %d!\" % (best_acc, best_epoch))\n",
        "\n",
        "        # save the current best model parameters\n",
        "        print(\"Save the best model checkpoint as `best_model.ckpt`!\")\n",
        "        # torch.save(model.state_dict(), \"best_model.ckpt\")\n",
        "        model.save_pretrained(\"best_bert_model.ckpt\")\n",
        "\n",
        "    print(\"Time elapsed: %s\" % time.strftime(\"%Hh%Mm%Ss\", time.gmtime(time.time()-start_time)))\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "# load back the best checkpoint on dev set\n",
        "print(\"End of training! The best accuracy %.3f was obtained in epoch %d.\" % (best_acc, best_epoch))\n",
        "# model.load_state_dict(torch.load(\"best_model.ckpt\"))\n",
        "model = BertForSequenceClassification.from_pretrained(\"best_bert_model.ckpt\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxjL3X9X-k29"
      },
      "source": [
        "By estimation, you should be able to complete the model training in 10 minutes (or significantly faster, depending on whether you used GPU or not; for my record, I finished training in 6 minutes when using the T4 GPU card in Colab.)\n",
        "\n",
        "Report the best performance of your model and the epoch you obtained it.\n",
        "How does the fine-tuned BERT model compare with (a) its counterpart without fine-tuning and (b) the feedforward neural network which we trained in the previous assignment? Why do you think the fine-tuned BERT could have the observed performance?\n",
        "\n",
        "<font color='blue'>YOUR TASK: provide your answer in the LaTex PDF.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRWacscV-k29"
      },
      "source": [
        "The following code will evaluate your model on the blind test set, with results saved in the data_to_submit folder. \n",
        "\n",
        "<font color='blue'>YOUR TASK: Submit your test output (i.e., your predictions) to Blackboard.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "__ZuzKQd-k29"
      },
      "outputs": [],
      "source": [
        "all_preds = [] # save the prediction results\n",
        "\n",
        "# iterator to load the test set\n",
        "eval_batch_iterator = SentimentExampleBatchIterator(test_exs_words_only, batch_size=32, shuffle=False)\n",
        "eval_batch_iterator.refresh()\n",
        "batch_data = eval_batch_iterator.get_next_batch(tokenizer)\n",
        "while batch_data is not None:\n",
        "    batch_inputs, _ = batch_data\n",
        "    preds = batch_predict(model, batch_inputs) # the `preds` shoud be a list of prediction labels\n",
        "    all_preds += preds # accumulate the labels\n",
        "    batch_data = eval_batch_iterator.get_next_batch(tokenizer)\n",
        "\n",
        "# write the predicted labels along with its original sentence\n",
        "test_output_path = \"H:\\\\Projects\\\\CS478\\\\Assignment4\\\\test-blind.bert-output.txt\"\n",
        "test_exs_predicted = [SentimentExample(ex.sentence, all_preds[ex_idx]) for ex_idx, ex in enumerate(test_exs_words_only)]\n",
        "write_sentiment_examples(test_exs_predicted, test_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiXm75w-k29"
      },
      "source": [
        "## You have completed this assignment! Please upload your notebook and the test output file in `data_to_submit` to Blackboard. Complete the LaTex file with a shareable link to this notebook and upload the PDF to Gradescope.\n",
        "\n",
        "**Note: please do NOT upload any model checkpoint as it can be too large to be taken by the system.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "52a6295e16456078059e92487b3807f4984b19b09542b8f052beb2147b3192fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
